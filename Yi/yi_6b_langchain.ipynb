{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "def get_files(dir_path):\n",
    "    # args：dir_path，目标文件夹路径\n",
    "    file_list = []\n",
    "    for filepath, dirnames, filenames in os.walk(dir_path):\n",
    "        # os.walk 函数将递归遍历指定文件夹\n",
    "        for filename in filenames:\n",
    "            # 通过后缀名判断文件类型是否满足要求\n",
    "            if filename.endswith(\".md\"):\n",
    "                # 如果满足要求，将其绝对路径加入到结果列表\n",
    "                file_list.append(os.path.join(filepath, filename))\n",
    "            elif filename.endswith(\".txt\"):\n",
    "                file_list.append(os.path.join(filepath, filename))\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "def get_text(dir_path):\n",
    "    # args：dir_path，目标文件夹路径\n",
    "    # 首先调用上文定义的函数得到目标文件路径列表\n",
    "    file_lst = get_files(dir_path)\n",
    "    # docs 存放加载之后的纯文本对象\n",
    "    docs = []\n",
    "    # 遍历所有目标文件\n",
    "    for one_file in tqdm(file_lst):\n",
    "        file_type = one_file.split('.')[-1]\n",
    "        if file_type == 'md':\n",
    "            loader = UnstructuredMarkdownLoader(one_file)\n",
    "        elif file_type == 'txt':\n",
    "            loader = UnstructuredFileLoader(one_file)\n",
    "        else:\n",
    "            # 如果是不符合条件的文件，直接跳过\n",
    "            continue\n",
    "        docs.extend(loader.load())\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/miniconda3/envs/mdllm/lib/python3.8/site-packages/unstructured/documents/html.py:498: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  rows = body.findall(\"tr\") if body else []\n",
      "100%|██████████| 68/68 [00:25<00:00,  2.68it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = get_text('/root/autodl-tmp/sweettalk-django4.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 引入langchain构建向量数据库。\n",
    "纯文本构建向量数据库， 首先对文本进行分块， 然后对文本向量化处理\n",
    "langchain提供了很多文本分块工具， 此处使用字符串递归分割器， \n",
    "选择分块大小500， 块堆叠长度150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=150)\n",
    "split_docs = text_splitter.split_documents(docs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用开源词向量模型 sentence transformer来进行文本向量化\n",
    "langchain提供了直接引入Huggingface开源社区中的模型进行向量化的接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mdllm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/mdllm/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/root/miniconda3/envs/mdllm/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/root/miniconda3/envs/mdllm/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"/root/autodl-tmp/embedding_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择chroma作为向量数据库， 基于上文分块后的文档及加载的开源向量化模型， 将预料加载到指定路径下的向量数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# 定义持久化路径\n",
    "persist_directory = 'data_base/vector_db/chroma'\n",
    "# 加载数据库\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上\n",
    ")\n",
    "# 将加载的向量数据库持久化到磁盘上\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yi接入langchain\n",
    "为方便构建LLM应用， 需要基于本地部署的YiLM, 自定义一个LLM类，将Yi接入到LangChain框架中。完成自定义LLM类之后， 可以以完全一致的方式调用LangChain的接口， 而无需考虑底层模型调用的不一致。\n",
    "本地部署的Yi自定义LLM类并不复杂， 只需从LangChain.llms.base.LLM类继承一个子类， 并重写构造函数与_call函数即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Any, List, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, LlamaTokenizerFast\n",
    "import torch\n",
    "\n",
    "class Yi_LLM(LLM):\n",
    "    # 基于本地 Yi 自定义 LLM 类\n",
    "    tokenizer: AutoTokenizer = None\n",
    "    model: AutoModelForCausalLM = None\n",
    "        \n",
    "    def __init__(self, mode_name_or_path :str):\n",
    "\n",
    "        super().__init__()\n",
    "        print(\"正在从本地加载模型...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, trust_remote_code=True, use_fast=False)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(mode_name_or_path, trust_remote_code=True,torch_dtype=torch.bfloat16,device_map=\"auto\")\n",
    "        self.model.generation_config = GenerationConfig.from_pretrained(mode_name_or_path)\n",
    "        self.model.generation_config.pad_token_id = self.model.generation_config.eos_token_id\n",
    "        self.model = self.model.eval()\n",
    "        print(\"完成本地模型的加载\")\n",
    "        \n",
    "    def _call(self, prompt : str, stop: Optional[List[str]] = None,\n",
    "                run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "                **kwargs: Any):\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt }\n",
    "                    ]\n",
    "        input_ids = self.tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "    \n",
    "        output_ids = self.model.generate(input_ids.to('cuda'))\n",
    "        response = self.tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        return response\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"Yi_LLM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建检索问答链\n",
    "LangChain通过提供检索问答链对象来实现对RAG全流程的封装。即我们可以调用一个LangChain提供的RetrievalQA， 通过初始化时填入已构建的数据库和自定义LLM作为参数， 来简便地完成检索增强问答的全流程。LangChain会自动完成基于用户提供进行检索， 获取相关文档， 拼接为合适的Prompt并交给LLM问答的全部流程。\n",
    "首先导入通过上下文构建的向量数据库， 通过Chroma以及上文定义的词向量模型来加载已构建的数据库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# 定义 Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"/root/autodl-tmp/embedding_model\")\n",
    "\n",
    "# 向量数据库持久化路径\n",
    "persist_directory = 'data_base/vector_db/chroma'\n",
    "\n",
    "# 加载数据库\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory, \n",
    "    embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从本地加载模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.12it/s]\n",
      "/root/miniconda3/envs/mdllm/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成本地模型的加载\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你好！我是零一万物开发的智能助手，我叫 Yi，我是由工程师们通过大量的文本数据进行训练的，旨在为用户提供有用的信息和帮助解答问题。请问有什么我可以帮助你的？'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Yi_LLM(mode_name_or_path = \"/root/autodl-tmp/01ai/Yi-6B-Chat\")\n",
    "llm(\"你是谁\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建检索问答链，还需要构建一个 Prompt Template，该 Template 其实基于一个带变量的字符串，在检索之后，LangChain 会将检索到的相关文档片段填入到 Template 的变量中，从而实现带知识的 Prompt 构建。我们可以基于 LangChain 的 Template 基类来实例化这样一个 Template 对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 我们所构造的 Prompt 模板\n",
    "template = \"\"\"使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答案。尽量使答案简明扼要。总是在回答的最后说“谢谢你的提问！”。\n",
    "{context}\n",
    "问题: {question}\n",
    "有用的回答:\"\"\"\n",
    "\n",
    "# 调用 LangChain 的方法来实例化一个 Template 对象，该对象包含了 context 和 question 两个变量，在实际调用时，这两个变量会被检索到的文档片段和用户提问填充\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，可以调用 LangChain 提供的检索问答链构造函数，基于我们的自定义 LLM、Prompt Template 和向量知识库来构建一个基于 Yi 的检索问答链："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mdllm/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索问答链回答 question 的结果：\n",
      "Sweettalk_Django是一个Python框架，它提供了一个易于使用的API，允许开发者轻松地创建和管理基于Web的应用程序。该框架建立在Django之上，提供了一系列的扩展和工具，以帮助开发者更快地开发出高质量的应用程序。\n"
     ]
    }
   ],
   "source": [
    "question = \"sweettalk_django项目是什么\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(\"检索问答链回答 question 的结果：\")\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

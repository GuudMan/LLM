{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mdllm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/mdllm/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/root/miniconda3/envs/mdllm/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将JSON文件转换为CSV文件\n",
    "df = pd.read_json('./huanhuan.json')\n",
    "ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QWenTokenizer(name_or_path='./qwen/Qwen-7B-Chat/', vocab_size=151851, model_max_length=8192, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./qwen/Qwen-7B-Chat/', use_fast=False, trust_remote_code=True)\n",
    "tokenizer.pad_token_id = tokenizer.eod_id\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 384    # Llama分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(\"\\n\".join([\"<|im_start|>system\", \"现在你要扮演皇帝身边的女人--甄嬛.<|im_end|>\" + \"\\n<|im_start|>user\\n\" + example[\"instruction\"] + example[\"input\"] + \"<|im_end|>\\n\"]).strip(), add_special_tokens=False)  # add_special_tokens 不在开头加 special_tokens\n",
    "    response = tokenizer(\"<|im_start|>assistant\\n\" + example[\"output\"] + \"<|im_end|>\\n\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # 因为eos token咱们也是要关注的所以 补充为1\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  # Qwen的特殊构造就是这样的\n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3729/3729 [00:01<00:00, 3246.71 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3729\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: please make sure that you are using the latest codes and checkpoints, especially if you used Qwen-7B before 09.25.2023.请使用最新模型和代码，尤其如果你在9月25日前已经开始使用Qwen-7B，千万注意不要使用错误代码和模型。\n",
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:05<00:00,  1.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QWenLMHeadModel(\n",
       "  (transformer): QWenModel(\n",
       "    (wte): Embedding(151936, 4096)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (rotary_emb): RotaryEmbedding()\n",
       "    (h): ModuleList(\n",
       "      (0-31): 32 x QWenBlock(\n",
       "        (ln_1): RMSNorm()\n",
       "        (attn): QWenAttention(\n",
       "          (c_attn): Linear8bitLt(in_features=4096, out_features=12288, bias=True)\n",
       "          (c_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (core_attention_flash): FlashSelfAttention()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): RMSNorm()\n",
       "        (mlp): QWenMLP(\n",
       "          (w1): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (w2): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (c_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('./qwen/Qwen-7B-Chat/', trust_remote_code=True, torch_dtype=torch.half, load_in_8bit=True, device_map=\"sequential\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([151936, 4096]) torch.float16\n",
      "transformer.h.0.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.0.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.0.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.0.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.1.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.1.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.1.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.1.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.2.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.2.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.2.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.2.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.3.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.3.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.3.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.3.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.4.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.4.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.4.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.4.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.5.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.5.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.5.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.5.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.6.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.6.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.6.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.6.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.7.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.7.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.7.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.7.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.8.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.8.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.8.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.8.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.9.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.9.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.9.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.9.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.10.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.10.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.10.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.10.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.11.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.11.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.11.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.11.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.12.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.12.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.12.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.12.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.12.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.12.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.12.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.12.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.13.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.13.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.13.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.13.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.13.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.13.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.13.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.13.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.14.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.14.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.14.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.14.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.14.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.14.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.14.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.14.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.15.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.15.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.15.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.15.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.15.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.15.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.15.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.15.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.16.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.16.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.16.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.16.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.16.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.16.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.16.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.16.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.17.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.17.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.17.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.17.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.17.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.17.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.17.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.17.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.18.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.18.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.18.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.18.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.18.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.18.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.18.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.18.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.19.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.19.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.19.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.19.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.19.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.19.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.19.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.19.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.20.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.20.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.20.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.20.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.20.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.20.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.20.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.20.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.21.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.21.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.21.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.21.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.21.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.21.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.21.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.21.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.22.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.22.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.22.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.22.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.22.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.22.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.22.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.22.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.23.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.23.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.23.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.23.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.23.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.23.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.23.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.23.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.24.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.24.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.24.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.24.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.24.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.24.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.24.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.24.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.25.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.25.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.25.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.25.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.25.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.25.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.25.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.25.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.26.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.26.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.26.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.26.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.26.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.26.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.26.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.26.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.27.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.27.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.27.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.27.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.27.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.27.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.27.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.27.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.28.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.28.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.28.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.28.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.28.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.28.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.28.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.28.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.29.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.29.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.29.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.29.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.29.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.29.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.29.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.29.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.30.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.30.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.30.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.30.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.30.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.30.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.30.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.30.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.h.31.ln_1.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.31.attn.c_attn.weight torch.Size([12288, 4096]) torch.int8\n",
      "transformer.h.31.attn.c_attn.bias torch.Size([12288]) torch.float16\n",
      "transformer.h.31.attn.c_proj.weight torch.Size([4096, 4096]) torch.int8\n",
      "transformer.h.31.ln_2.weight torch.Size([4096]) torch.float16\n",
      "transformer.h.31.mlp.w1.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.31.mlp.w2.weight torch.Size([11008, 4096]) torch.int8\n",
      "transformer.h.31.mlp.c_proj.weight torch.Size([4096, 11008]) torch.int8\n",
      "transformer.ln_f.weight torch.Size([4096]) torch.float16\n",
      "lm_head.weight torch.Size([151936, 4096]) torch.float16\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.transformer.wte.weight\n",
      "base_model.model.transformer.h.0.ln_1.weight\n",
      "base_model.model.transformer.h.0.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.0.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.0.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.0.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.0.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.0.ln_2.weight\n",
      "base_model.model.transformer.h.0.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.0.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.0.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.0.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.0.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.0.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.0.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.0.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.0.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.1.ln_1.weight\n",
      "base_model.model.transformer.h.1.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.1.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.1.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.1.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.1.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.1.ln_2.weight\n",
      "base_model.model.transformer.h.1.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.1.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.1.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.1.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.1.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.1.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.1.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.1.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.1.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.2.ln_1.weight\n",
      "base_model.model.transformer.h.2.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.2.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.2.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.2.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.2.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.2.ln_2.weight\n",
      "base_model.model.transformer.h.2.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.2.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.2.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.2.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.2.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.2.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.2.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.2.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.2.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.3.ln_1.weight\n",
      "base_model.model.transformer.h.3.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.3.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.3.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.3.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.3.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.3.ln_2.weight\n",
      "base_model.model.transformer.h.3.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.3.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.3.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.3.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.3.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.3.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.3.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.3.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.3.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.4.ln_1.weight\n",
      "base_model.model.transformer.h.4.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.4.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.4.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.4.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.4.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.4.ln_2.weight\n",
      "base_model.model.transformer.h.4.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.4.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.4.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.4.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.4.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.4.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.4.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.4.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.4.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.5.ln_1.weight\n",
      "base_model.model.transformer.h.5.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.5.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.5.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.5.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.5.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.5.ln_2.weight\n",
      "base_model.model.transformer.h.5.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.5.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.5.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.5.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.5.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.5.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.5.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.5.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.5.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.6.ln_1.weight\n",
      "base_model.model.transformer.h.6.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.6.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.6.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.6.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.6.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.6.ln_2.weight\n",
      "base_model.model.transformer.h.6.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.6.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.6.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.6.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.6.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.6.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.6.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.6.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.6.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.7.ln_1.weight\n",
      "base_model.model.transformer.h.7.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.7.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.7.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.7.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.7.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.7.ln_2.weight\n",
      "base_model.model.transformer.h.7.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.7.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.7.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.7.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.7.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.7.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.7.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.7.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.7.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.8.ln_1.weight\n",
      "base_model.model.transformer.h.8.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.8.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.8.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.8.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.8.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.8.ln_2.weight\n",
      "base_model.model.transformer.h.8.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.8.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.8.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.8.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.8.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.8.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.8.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.8.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.8.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.9.ln_1.weight\n",
      "base_model.model.transformer.h.9.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.9.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.9.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.9.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.9.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.9.ln_2.weight\n",
      "base_model.model.transformer.h.9.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.9.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.9.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.9.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.9.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.9.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.9.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.9.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.9.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.10.ln_1.weight\n",
      "base_model.model.transformer.h.10.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.10.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.10.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.10.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.10.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.10.ln_2.weight\n",
      "base_model.model.transformer.h.10.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.10.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.10.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.10.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.10.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.10.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.10.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.10.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.10.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.11.ln_1.weight\n",
      "base_model.model.transformer.h.11.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.11.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.11.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.11.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.11.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.11.ln_2.weight\n",
      "base_model.model.transformer.h.11.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.11.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.11.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.11.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.11.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.11.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.11.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.11.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.11.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.12.ln_1.weight\n",
      "base_model.model.transformer.h.12.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.12.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.12.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.12.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.12.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.12.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.12.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.12.ln_2.weight\n",
      "base_model.model.transformer.h.12.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.12.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.12.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.12.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.12.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.12.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.12.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.12.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.12.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.13.ln_1.weight\n",
      "base_model.model.transformer.h.13.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.13.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.13.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.13.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.13.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.13.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.13.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.13.ln_2.weight\n",
      "base_model.model.transformer.h.13.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.13.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.13.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.13.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.13.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.13.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.13.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.13.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.13.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.14.ln_1.weight\n",
      "base_model.model.transformer.h.14.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.14.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.14.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.14.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.14.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.14.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.14.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.14.ln_2.weight\n",
      "base_model.model.transformer.h.14.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.14.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.14.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.14.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.14.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.14.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.14.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.14.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.14.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.15.ln_1.weight\n",
      "base_model.model.transformer.h.15.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.15.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.15.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.15.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.15.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.15.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.15.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.15.ln_2.weight\n",
      "base_model.model.transformer.h.15.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.15.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.15.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.15.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.15.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.15.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.15.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.15.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.15.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.16.ln_1.weight\n",
      "base_model.model.transformer.h.16.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.16.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.16.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.16.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.16.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.16.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.16.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.16.ln_2.weight\n",
      "base_model.model.transformer.h.16.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.16.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.16.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.16.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.16.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.16.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.16.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.16.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.16.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.17.ln_1.weight\n",
      "base_model.model.transformer.h.17.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.17.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.17.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.17.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.17.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.17.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.17.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.17.ln_2.weight\n",
      "base_model.model.transformer.h.17.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.17.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.17.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.17.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.17.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.17.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.17.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.17.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.17.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.18.ln_1.weight\n",
      "base_model.model.transformer.h.18.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.18.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.18.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.18.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.18.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.18.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.18.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.18.ln_2.weight\n",
      "base_model.model.transformer.h.18.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.18.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.18.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.18.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.18.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.18.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.18.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.18.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.18.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.19.ln_1.weight\n",
      "base_model.model.transformer.h.19.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.19.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.19.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.19.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.19.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.19.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.19.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.19.ln_2.weight\n",
      "base_model.model.transformer.h.19.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.19.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.19.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.19.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.19.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.19.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.19.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.19.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.19.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.20.ln_1.weight\n",
      "base_model.model.transformer.h.20.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.20.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.20.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.20.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.20.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.20.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.20.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.20.ln_2.weight\n",
      "base_model.model.transformer.h.20.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.20.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.20.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.20.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.20.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.20.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.20.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.20.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.20.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.21.ln_1.weight\n",
      "base_model.model.transformer.h.21.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.21.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.21.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.21.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.21.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.21.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.21.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.21.ln_2.weight\n",
      "base_model.model.transformer.h.21.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.21.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.21.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.21.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.21.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.21.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.21.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.21.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.21.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.22.ln_1.weight\n",
      "base_model.model.transformer.h.22.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.22.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.22.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.22.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.22.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.22.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.22.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.22.ln_2.weight\n",
      "base_model.model.transformer.h.22.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.22.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.22.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.22.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.22.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.22.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.22.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.22.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.22.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.23.ln_1.weight\n",
      "base_model.model.transformer.h.23.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.23.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.23.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.23.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.23.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.23.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.23.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.23.ln_2.weight\n",
      "base_model.model.transformer.h.23.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.23.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.23.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.23.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.23.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.23.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.23.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.23.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.23.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.24.ln_1.weight\n",
      "base_model.model.transformer.h.24.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.24.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.24.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.24.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.24.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.24.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.24.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.24.ln_2.weight\n",
      "base_model.model.transformer.h.24.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.24.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.24.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.24.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.24.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.24.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.24.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.24.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.24.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.25.ln_1.weight\n",
      "base_model.model.transformer.h.25.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.25.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.25.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.25.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.25.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.25.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.25.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.25.ln_2.weight\n",
      "base_model.model.transformer.h.25.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.25.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.25.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.25.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.25.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.25.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.25.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.25.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.25.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.26.ln_1.weight\n",
      "base_model.model.transformer.h.26.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.26.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.26.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.26.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.26.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.26.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.26.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.26.ln_2.weight\n",
      "base_model.model.transformer.h.26.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.26.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.26.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.26.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.26.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.26.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.26.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.26.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.26.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.27.ln_1.weight\n",
      "base_model.model.transformer.h.27.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.27.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.27.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.27.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.27.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.27.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.27.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.27.ln_2.weight\n",
      "base_model.model.transformer.h.27.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.27.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.27.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.27.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.27.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.27.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.27.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.27.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.27.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.28.ln_1.weight\n",
      "base_model.model.transformer.h.28.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.28.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.28.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.28.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.28.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.28.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.28.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.28.ln_2.weight\n",
      "base_model.model.transformer.h.28.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.28.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.28.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.28.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.28.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.28.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.28.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.28.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.28.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.29.ln_1.weight\n",
      "base_model.model.transformer.h.29.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.29.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.29.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.29.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.29.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.29.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.29.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.29.ln_2.weight\n",
      "base_model.model.transformer.h.29.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.29.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.29.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.29.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.29.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.29.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.29.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.29.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.29.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.30.ln_1.weight\n",
      "base_model.model.transformer.h.30.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.30.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.30.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.30.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.30.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.30.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.30.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.30.ln_2.weight\n",
      "base_model.model.transformer.h.30.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.30.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.30.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.30.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.30.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.30.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.30.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.30.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.30.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.31.ln_1.weight\n",
      "base_model.model.transformer.h.31.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.31.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.31.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.31.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.31.attn.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.31.attn.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.31.attn.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.h.31.ln_2.weight\n",
      "base_model.model.transformer.h.31.mlp.w1.base_layer.weight\n",
      "base_model.model.transformer.h.31.mlp.w1.lora_A.default.weight\n",
      "base_model.model.transformer.h.31.mlp.w1.lora_B.default.weight\n",
      "base_model.model.transformer.h.31.mlp.w2.base_layer.weight\n",
      "base_model.model.transformer.h.31.mlp.w2.lora_A.default.weight\n",
      "base_model.model.transformer.h.31.mlp.w2.lora_B.default.weight\n",
      "base_model.model.transformer.h.31.mlp.c_proj.base_layer.weight\n",
      "base_model.model.transformer.h.31.mlp.c_proj.lora_A.default.weight\n",
      "base_model.model.transformer.h.31.mlp.c_proj.lora_B.default.weight\n",
      "base_model.model.transformer.ln_f.weight\n",
      "base_model.model.lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'c_attn', 'c_proj', 'w1', 'w2'}, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"w1\", \"w2\"],\n",
    "    inference_mode=False, # 训练模式\n",
    "    r=8, # Lora 秩\n",
    "    lora_alpha=32, # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.1# Dropout 比例\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'c_attn', 'c_proj', 'w1', 'w2'}, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_peft_model(model, config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 17,891,328 || all params: 7,739,215,872 || trainable%: 0.23117752878207867\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./output/Qwen\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    gradient_checkpointing=True,\n",
    "    save_steps=100,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='348' max='348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [348/348 25:37, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.573700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.459600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.458100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.404100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.505700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.388600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.384900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.325200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.397800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.341100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.187700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.231900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.243700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.176600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.194200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.252100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.154400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.088100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.996200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.985900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.955800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.940700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.967600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.939400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.978800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.908800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./output/Qwen/checkpoint-100 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/root/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./output/Qwen/checkpoint-200 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/root/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./output/Qwen/checkpoint-300 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/root/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=348, training_loss=2.2234837323769754, metrics={'train_runtime': 1542.3373, 'train_samples_per_second': 7.253, 'train_steps_per_second': 0.226, 'total_flos': 5.381300986918502e+16, 'train_loss': 2.2234837323769754, 'epoch': 2.98})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      2\u001b[0m ipt \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|im_start|>system\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m现在你要扮演皇帝身边的女人--甄嬛.<|im_end|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m<|im_start|>user\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m<|im_end|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m你是谁？\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAssistant: \u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m----> 3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mipt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m], \n\u001b[1;32m      8\u001b[0m                                 skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/peft/peft_model.py:1190\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1189\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1190\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1192\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/peft/peft_model.py:1190\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1189\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1190\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1192\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/modeling_qwen.py:1318\u001b[0m, in \u001b[0;36mQWenLMHeadModel.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m         logits_processor\u001b[38;5;241m.\u001b[39mappend(stop_words_logits_processor)\n\u001b[0;32m-> 1318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43massistant_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massistant_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/transformers/generation/utils.py:1764\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1756\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1757\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1758\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1759\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1761\u001b[0m     )\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1781\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1782\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1788\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/transformers/generation/utils.py:2861\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2858\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2861\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2862\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2864\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2865\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2866\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2869\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/modeling_qwen.py:1108\u001b[0m, in \u001b[0;36mQWenLMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1087\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1088\u001b[0m     input_ids: Optional[torch\u001b[38;5;241m.\u001b[39mLongTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, CausalLMOutputWithPast]:\n\u001b[1;32m   1104\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1105\u001b[0m         return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1106\u001b[0m     )\n\u001b[0;32m-> 1108\u001b[0m     transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1125\u001b[0m     lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/modeling_qwen.py:938\u001b[0m, in \u001b[0;36mQWenModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    926\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    927\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    928\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    935\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    936\u001b[0m     )\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 938\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrotary_pos_emb_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrotary_pos_emb_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregistered_causal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregistered_causal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/modeling_qwen.py:639\u001b[0m, in \u001b[0;36mQWenBlock.forward\u001b[0;34m(self, hidden_states, rotary_pos_emb_list, registered_causal_mask, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    626\u001b[0m     hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    635\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    636\u001b[0m ):\n\u001b[1;32m    637\u001b[0m     layernorm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 639\u001b[0m     attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayernorm_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrotary_pos_emb_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregistered_causal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregistered_causal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    651\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/modeling_qwen.py:546\u001b[0m, in \u001b[0;36mQWenAttention.forward\u001b[0;34m(self, hidden_states, rotary_pos_emb_list, registered_causal_mask, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_flash_attn\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m flash_attn_unpadded_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fp32\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m query\u001b[38;5;241m.\u001b[39mis_cuda\n\u001b[1;32m    544\u001b[0m ):\n\u001b[1;32m    545\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m query, key, value\n\u001b[0;32m--> 546\u001b[0m     context_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_attention_flash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;66;03m# b s h d -> b s (h d)\u001b[39;00m\n\u001b[1;32m    549\u001b[0m     context_layer \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/modeling_qwen.py:174\u001b[0m, in \u001b[0;36mFlashSelfAttention.forward\u001b[0;34m(self, q, k, v, attention_mask)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, q, k, v, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m((i\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m [torch\u001b[38;5;241m.\u001b[39mfloat16, torch\u001b[38;5;241m.\u001b[39mbfloat16] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (q, k, v)))\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m((i\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (q, k, v)))\n\u001b[1;32m    176\u001b[0m     batch_size, seqlen_q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], q\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "ipt = tokenizer(\"<|im_start|>system\\n现在你要扮演皇帝身边的女人--甄嬛.<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n\".format(\"你是谁？\", \"\").strip() + \"\\nAssistant: \", return_tensors=\"pt\").to(model.device)\n",
    "tokenizer.decode(model.generate(**ipt, \n",
    "                                max_length=512, \n",
    "                                do_sample=True, \n",
    "                                eos_token_id=tokenizer.eos_token_id, \n",
    "                                temperature=0.1)[0], \n",
    "                                skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response, history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                               \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m今天晚上吃什么?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                               \u001b[49m\u001b[43msystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m现在你要扮演皇帝身边的女人--甄嬛.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m response\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/modeling_qwen.py:1199\u001b[0m, in \u001b[0;36mQWenLMHeadModel.chat\u001b[0;34m(self, tokenizer, query, history, system, append_history, stream, stop_words_ids, generation_config, **kwargs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m stop_words_ids\u001b[38;5;241m.\u001b[39mextend(get_stop_words_ids(\n\u001b[1;32m   1196\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mchat_format, tokenizer\n\u001b[1;32m   1197\u001b[0m ))\n\u001b[1;32m   1198\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([context_tokens])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1199\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop_words_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop_words_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1207\u001b[0m response \u001b[38;5;241m=\u001b[39m decode_tokens(\n\u001b[1;32m   1208\u001b[0m     outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1209\u001b[0m     tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1215\u001b[0m )\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m append_history:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/modeling_qwen.py:1318\u001b[0m, in \u001b[0;36mQWenLMHeadModel.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m         logits_processor\u001b[38;5;241m.\u001b[39mappend(stop_words_logits_processor)\n\u001b[0;32m-> 1318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43massistant_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massistant_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/transformers/generation/utils.py:1764\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1756\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1757\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1758\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1759\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1761\u001b[0m     )\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1781\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1782\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1788\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/transformers/generation/utils.py:2861\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2858\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2861\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2862\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2864\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2865\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2866\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2869\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/modeling_qwen.py:1108\u001b[0m, in \u001b[0;36mQWenLMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1087\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1088\u001b[0m     input_ids: Optional[torch\u001b[38;5;241m.\u001b[39mLongTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, CausalLMOutputWithPast]:\n\u001b[1;32m   1104\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1105\u001b[0m         return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1106\u001b[0m     )\n\u001b[0;32m-> 1108\u001b[0m     transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1125\u001b[0m     lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/modeling_qwen.py:938\u001b[0m, in \u001b[0;36mQWenModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    926\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    927\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    928\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    935\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    936\u001b[0m     )\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 938\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrotary_pos_emb_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrotary_pos_emb_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregistered_causal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregistered_causal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/modeling_qwen.py:639\u001b[0m, in \u001b[0;36mQWenBlock.forward\u001b[0;34m(self, hidden_states, rotary_pos_emb_list, registered_causal_mask, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    626\u001b[0m     hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    635\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    636\u001b[0m ):\n\u001b[1;32m    637\u001b[0m     layernorm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 639\u001b[0m     attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayernorm_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrotary_pos_emb_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregistered_causal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregistered_causal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    651\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/modeling_qwen.py:546\u001b[0m, in \u001b[0;36mQWenAttention.forward\u001b[0;34m(self, hidden_states, rotary_pos_emb_list, registered_causal_mask, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_flash_attn\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m flash_attn_unpadded_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fp32\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m query\u001b[38;5;241m.\u001b[39mis_cuda\n\u001b[1;32m    544\u001b[0m ):\n\u001b[1;32m    545\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m query, key, value\n\u001b[0;32m--> 546\u001b[0m     context_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_attention_flash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;66;03m# b s h d -> b s (h d)\u001b[39;00m\n\u001b[1;32m    549\u001b[0m     context_layer \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mdllm/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/modeling_qwen.py:174\u001b[0m, in \u001b[0;36mFlashSelfAttention.forward\u001b[0;34m(self, q, k, v, attention_mask)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, q, k, v, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m((i\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m [torch\u001b[38;5;241m.\u001b[39mfloat16, torch\u001b[38;5;241m.\u001b[39mbfloat16] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (q, k, v)))\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m((i\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (q, k, v)))\n\u001b[1;32m    176\u001b[0m     batch_size, seqlen_q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], q\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \n",
    "                               \"今天晚上吃什么?\", \n",
    "                               history=[], \n",
    "                               system=\"现在你要扮演皇帝身边的女人--甄嬛.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
